{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec4b30d",
   "metadata": {},
   "source": [
    "# Scikit-learn Pre-processing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73baad2e",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "Scikit-learn provides several methods for feature scaling, like `StandardScaler`, `MinMaxScaler`, `RobustScaler`, and `MaxAbsScaler`. \n",
    "\n",
    "| Common Methods | Description |\n",
    "|----------------|-------------|\n",
    "|`fit(X, y=None)`| Compute scaling parameters from training data |\n",
    "|`transform(X)` |Apply scaling transformation to data|\n",
    "|`fit_transform(X, y=None)`| Fit and transform in one step|\n",
    "|`inverse_transform(X)`| Reverse the scaling transformation|\n",
    "|`get_params(deep=True)`| Get parameters for this estimator|\n",
    "|`set_params(**params)`| Set parameters for this estimator |\n",
    "\n",
    "| Attribute | Description |\n",
    "|-----------|-------------|\n",
    "|`StandardScaler`| <table><tr><td>mean_</td><td>Mean of each feature</td></tr><tr><td>scale_</td><td>Standard deviation of each feature</td></tr><tr><td>var_</td><td>Variance of each feature</td></tr><tr><td>n_features_in_</td><td>Number of features seen during fit</td></tr><tr><td>feature_names_in_</td><td>Names of features seen during fit</td></tr></table> |\n",
    "|`MinMaxScaler`| <table><tr><td>data_min_</td><td>Minimum value of each feature</td></tr><tr><td>data_max_</td><td>Maximum value of each feature</td></tr><tr><td>data_range_</td><td>Range of each feature</td></tr><tr><td>min_</td><td>Minimum value of each feature after scaling</td></tr><tr><td>max_</td><td>Maximum value of each feature after scaling</td></tr><tr><td>scale_</td><td>Scale factor for each feature</td></tr><tr><td>n_features_in_</td><td>Number of features seen during fit</td></tr><tr><td>feature_names_in_</td><td>Names of features seen during fit</td></tr></table> |\n",
    "|`RobustScaler`| <table><tr><td>center_</td><td>Median of each feature</td></tr><tr><td>scale_</td><td>Interquartile range of each feature</td></tr><tr><td>n_features_in_</td><td>Number of features seen during fit</td></tr><tr><td>feature_names_in_</td><td>Names of features seen during fit</td></tr></table> |\n",
    "|`MaxAbsScaler`| <table><tr><td>max_abs_</td><td>Maximum absolute value of each feature after scaling</td></tr><tr><td>scale_</td><td>Maximum absolute value of each feature</td></tr><tr><td>n_features_in_</td><td>Number of features seen during fit</td></tr><tr><td>feature_names_in_</td><td>Names of features seen during fit</td></tr></table> |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a046a843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      " [[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "Standardized Data:\n",
      " [[-1.22474487 -1.22474487 -1.22474487]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 1.22474487  1.22474487  1.22474487]]\n",
      "Min-Max Scaled Data:\n",
      " [[0.  0.  0. ]\n",
      " [0.5 0.5 0.5]\n",
      " [1.  1.  1. ]]\n",
      "Robust Scaled Data:\n",
      " [[-1. -1. -1.]\n",
      " [ 0.  0.  0.]\n",
      " [ 1.  1.  1.]]\n",
      "MaxAbs Scaled Data:\n",
      " [[0.14285714 0.25       0.33333333]\n",
      " [0.57142857 0.625      0.66666667]\n",
      " [1.         1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(\"Original Data:\\n\", data)\n",
    "\n",
    "# Standardization\n",
    "scaler_standard = StandardScaler()\n",
    "data_standardized = scaler_standard.fit_transform(data)\n",
    "print(\"Standardized Data:\\n\", data_standardized)\n",
    "\n",
    "# Min-Max Scaling\n",
    "scaler_minmax = MinMaxScaler()\n",
    "data_minmax = scaler_minmax.fit_transform(data)\n",
    "print(\"Min-Max Scaled Data:\\n\", data_minmax)\n",
    "\n",
    "# Robust Scaling\n",
    "scaler_robust = RobustScaler()\n",
    "data_robust = scaler_robust.fit_transform(data)\n",
    "print(\"Robust Scaled Data:\\n\", data_robust)\n",
    "\n",
    "# MaxAbs Scaling\n",
    "scaler_maxabs = MaxAbsScaler()\n",
    "data_maxabs = scaler_maxabs.fit_transform(data)\n",
    "print(\"MaxAbs Scaled Data:\\n\", data_maxabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04f160a",
   "metadata": {},
   "source": [
    "## Feature encoding\n",
    "Scikit-learn provides several methods for feature encoding, like `LabelEncoder`, `OrdinalEncoder` and `OneHotEncoder`\n",
    "| Common Methods | Description |\n",
    "|----------------|-------------|\n",
    "|`fit(X, y=None)`| Compute encoding parameters from training data |\n",
    "|`transform(X)` |Apply encoding transformation to data|\n",
    "|`fit_transform(X, y=None)`| Fit and transform in one step|\n",
    "|`inverse_transform(X)`| Reverse the encoding transformation|\n",
    "|`get_params(deep=True)`| Get parameters for this estimator|\n",
    "|`set_params(**params)`| Set parameters for this estimator |\n",
    "\n",
    "| Attribute | Description |\n",
    "|-----------|-------------|\n",
    "|`LabelEncoder`| <table><tr><td>classes_</td><td>Unique classes in the data</td></tr><tr><td>n_classes_</td><td>Number of unique classes</td></tr></table> |\n",
    "|`OrdinalEncoder`| <table><tr><td>categories_</td><td>Categories for each feature</td></tr><tr><td>n_features_in_</td><td>Number of features seen during fit</td></tr><tr><td>feature_names_in_</td><td>Names of features seen during fit</td></tr></table> |\n",
    "|`OneHotEncoder`| <table><tr><td>categories_</td><td>Categories for each feature</td></tr><tr><td>n_values_</td><td>Number of unique values for each feature</td></tr><tr><td>n_features_in_</td><td>Number of features seen during fit</td></tr><tr><td>feature_names_in_</td><td>Names of features seen during fit</td></tr></table> |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4939e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orignal Data:\n",
      "        0       1\n",
      "0    red   small\n",
      "1   blue   large\n",
      "2  green  medium\n",
      "Encoded Labels:\n",
      " [2 0 1]\n",
      "Ordinal Encoded Data:\n",
      " [[0. 0.]\n",
      " [1. 2.]\n",
      " [2. 1.]]\n",
      "OneHot Encoded Sparse Data:\n",
      "   (0, 2)\t1.0\n",
      "  (0, 5)\t1.0\n",
      "  (1, 0)\t1.0\n",
      "  (1, 3)\t1.0\n",
      "  (2, 1)\t1.0\n",
      "  (2, 4)\t1.0\n",
      "OneHot Encoded Dense Data:\n",
      " [[0. 0. 1. 0. 0. 1.]\n",
      " [1. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0.]]\n",
      "Dummy Encoded Data:\n",
      " [[0. 1.]\n",
      " [0. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Example categorical data\n",
    "categorical_data = pd.DataFrame([['red', 'small'], ['blue', 'large'], ['green', 'medium']])\n",
    "print(\"Orignal Data:\\n\", categorical_data)\n",
    "\n",
    "# LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(categorical_data[0]) \n",
    "print(\"Encoded Labels:\\n\", encoded_labels)\n",
    "\n",
    "# OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder(categories=[['red', 'blue', 'green'], ['small', 'medium', 'large']]) #specifying the order of categories\n",
    "ordinal_encoded = ordinal_encoder.fit_transform(categorical_data)\n",
    "print(\"Ordinal Encoded Data:\\n\", ordinal_encoded)\n",
    "\n",
    "# OneHotEncoder\n",
    "onehot_encoder_sparse = OneHotEncoder(sparse_output=True) # sparse_output=True returns a sparse matrix\n",
    "onehot_encoded_sparse = onehot_encoder_sparse.fit_transform(categorical_data)\n",
    "onehot_encoder_dense = OneHotEncoder(sparse_output=False) # sparse_output=False returns a dense matrix\n",
    "onehot_encoded_dense = onehot_encoder_dense.fit_transform(categorical_data)\n",
    "print(\"OneHot Encoded Sparse Data:\\n\", onehot_encoded_sparse)\n",
    "print(\"OneHot Encoded Dense Data:\\n\", onehot_encoded_dense)\n",
    "\n",
    "# Dummy Encoding\n",
    "dummy_encoder = OneHotEncoder(drop='first', sparse_output=False) # drop='first' drops the first category to archieve dummy encoding\n",
    "dummy_encoded = dummy_encoder.fit_transform(categorical_data[[0]])\n",
    "print(\"Dummy Encoded Data:\\n\", dummy_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e58f49",
   "metadata": {},
   "source": [
    "## Missing Value Imputation\n",
    "\n",
    "Scikit-learn provides several methods for handling missing values through the `sklearn.impute` module.\n",
    "\n",
    "| Common Methods | Description |\n",
    "|----------------|-------------|\n",
    "|`fit(X, y=None)`| Learn imputation strategy from training data |\n",
    "|`transform(X)` |Apply imputation to data|\n",
    "|`fit_transform(X, y=None)`| Fit and transform in one step|\n",
    "|`get_params(deep=True)`| Get parameters for this estimator|\n",
    "|`set_params(**params)`| Set parameters for this estimator |\n",
    "\n",
    "  \n",
    "\n",
    "| Imputer | Description | Key Parameters |\n",
    "|---------|-------------|----------------|\n",
    "|`SimpleImputer`| Basic imputation strategies | `strategy`: 'mean', 'median', 'most_frequent', 'constant'<br>`fill_value`: Value for constant strategy |\n",
    "|`IterativeImputer`| Multivariate imputation using other features | `estimator`: Model to use for prediction,default is `BayesianRidge()`<br>`max_iter`: Maximum iterations<br>`random_state`: For reproducibility |\n",
    "|`KNNImputer`| K-Nearest Neighbors imputation | `n_neighbors`: Number of neighbors<br>`weights`: 'uniform' or 'distance' |\n",
    "|`MissingIndicator`| Creates binary indicators for missing values | `features`: 'missing-only' or 'all'<br>`sparse`: Return sparse matrix or not |\n",
    "\n",
    "| Attributes | SimpleImputer | IterativeImputer | KNNImputer |\n",
    "|-----------|---------------|------------------|------------|\n",
    "|`statistics_`| Imputation values per feature | ✓ | ✗ | ✗ |\n",
    "|`indicator_`| MissingIndicator object | ✓ | ✓ | ✓ |\n",
    "|`n_features_in_`| Number of features seen during fit | ✓ | ✓ | ✓ |\n",
    "|`feature_names_in_`| Names of features seen during fit | ✓ | ✓ | ✓ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e1c8ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data with Missing Values:\n",
      "      A     B     C     D\n",
      "0   1.0   2.0   3.0   4.0\n",
      "1   5.0   NaN   7.0   8.0\n",
      "2   9.0  10.0   NaN  12.0\n",
      "3  13.0  14.0  15.0   NaN\n",
      "4   NaN  18.0  19.0  20.0\n",
      "5  21.0  22.0  23.0  24.0\n",
      "\n",
      "=== SimpleImputer Examples ===\n",
      "\n",
      "Mean Imputation:\n",
      "      A     B     C     D\n",
      "0   1.0   2.0   3.0   4.0\n",
      "1   5.0  13.2   7.0   8.0\n",
      "2   9.0  10.0  13.4  12.0\n",
      "3  13.0  14.0  15.0  13.6\n",
      "4   9.8  18.0  19.0  20.0\n",
      "5  21.0  22.0  23.0  24.0\n",
      "Imputation values: [ 9.8 13.2 13.4 13.6]\n",
      "\n",
      "Median Imputation:\n",
      "      A     B     C     D\n",
      "0   1.0   2.0   3.0   4.0\n",
      "1   5.0  14.0   7.0   8.0\n",
      "2   9.0  10.0  15.0  12.0\n",
      "3  13.0  14.0  15.0  12.0\n",
      "4   9.0  18.0  19.0  20.0\n",
      "5  21.0  22.0  23.0  24.0\n",
      "\n",
      "Constant Imputation (999):\n",
      "       A      B      C      D\n",
      "0    1.0    2.0    3.0    4.0\n",
      "1    5.0  999.0    7.0    8.0\n",
      "2    9.0   10.0  999.0   12.0\n",
      "3   13.0   14.0   15.0  999.0\n",
      "4  999.0   18.0   19.0   20.0\n",
      "5   21.0   22.0   23.0   24.0\n",
      "\n",
      "=== IterativeImputer Example ===\n",
      "Default Iterative Imputation:\n",
      "           A          B          C     D\n",
      "0   1.000000   2.000000   3.000000   4.0\n",
      "1   5.000000   6.000004   7.000000   8.0\n",
      "2   9.000000  10.000000  10.999999  12.0\n",
      "3  13.000000  14.000000  15.000000  16.0\n",
      "4  17.000001  18.000000  19.000000  20.0\n",
      "5  21.000000  22.000000  23.000000  24.0\n",
      "\n",
      "Iterative Imputation with DecisionTreeRegressor:\n",
      "      A     B     C     D\n",
      "0   1.0   2.0   3.0   4.0\n",
      "1   5.0   2.0   7.0   8.0\n",
      "2   9.0  10.0  15.0  12.0\n",
      "3  13.0  14.0  15.0  12.0\n",
      "4  13.0  18.0  19.0  20.0\n",
      "5  21.0  22.0  23.0  24.0\n",
      "\n",
      "=== KNNImputer Example ===\n",
      "KNN Imputation (k=2):\n",
      "      A     B     C     D\n",
      "0   1.0   2.0   3.0   4.0\n",
      "1   5.0   6.0   7.0   8.0\n",
      "2   9.0  10.0  11.0  12.0\n",
      "3  13.0  14.0  15.0  16.0\n",
      "4  17.0  18.0  19.0  20.0\n",
      "5  21.0  22.0  23.0  24.0\n",
      "\n",
      "=== MissingIndicator Example ===\n",
      "Missing Value Indicators:\n",
      "   missing_A  missing_B  missing_C  missing_D\n",
      "0      False      False      False      False\n",
      "1      False       True      False      False\n",
      "2      False      False       True      False\n",
      "3      False      False      False       True\n",
      "4       True      False      False      False\n",
      "5      False      False      False      False\n",
      "Features with missing values: [0 1 2 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\anaconda3\\envs\\nlp_img\\lib\\site-packages\\sklearn\\impute\\_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create sample data with missing values\n",
    "np.random.seed(42)\n",
    "data = np.array([\n",
    "    [1, 2, 3, 4],\n",
    "    [5, np.nan, 7, 8],\n",
    "    [9, 10, np.nan, 12],\n",
    "    [13, 14, 15, np.nan],\n",
    "    [np.nan, 18, 19, 20],\n",
    "    [21, 22, 23, 24]\n",
    "])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['A', 'B', 'C', 'D'])\n",
    "print(\"Original Data with Missing Values:\")\n",
    "print(df)\n",
    "\n",
    "# 1. SimpleImputer - Different strategies\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "print(\"\\n=== SimpleImputer Examples ===\")\n",
    "\n",
    "# Mean imputation\n",
    "imputer_mean = SimpleImputer(strategy='mean')\n",
    "data_mean = imputer_mean.fit_transform(data)\n",
    "print(f\"\\nMean Imputation:\")\n",
    "print(pd.DataFrame(data_mean, columns=['A', 'B', 'C', 'D']))\n",
    "print(f\"Imputation values: {imputer_mean.statistics_}\")\n",
    "\n",
    "# Median imputation\n",
    "imputer_median = SimpleImputer(strategy='median')\n",
    "data_median = imputer_median.fit_transform(data)\n",
    "print(f\"\\nMedian Imputation:\")\n",
    "print(pd.DataFrame(data_median, columns=['A', 'B', 'C', 'D']))\n",
    "\n",
    "# Constant value imputation\n",
    "imputer_constant = SimpleImputer(strategy='constant', fill_value=999)\n",
    "data_constant = imputer_constant.fit_transform(data)\n",
    "print(f\"\\nConstant Imputation (999):\")\n",
    "print(pd.DataFrame(data_constant, columns=['A', 'B', 'C', 'D']))\n",
    "\n",
    "# 2. IterativeImputer - Multivariate imputation\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "print(f\"\\n=== IterativeImputer Example ===\")\n",
    "# Using BayesianRidge as default estimator\n",
    "# You can also specify other estimators like DecisionTreeRegressor, RandomForestRegressor, etc.\n",
    "imputer_iterative_default = IterativeImputer(random_state=42, max_iter=10)\n",
    "data_iterative_default = imputer_iterative_default.fit_transform(data)\n",
    "print(f\"Default Iterative Imputation:\")\n",
    "print(pd.DataFrame(data_iterative_default, columns=['A', 'B', 'C', 'D']))\n",
    "# Using a different estimator (e.g., DecisionTreeRegressor)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "imputer_iterative_tree = IterativeImputer(estimator=DecisionTreeRegressor(), random_state=42, max_iter=10)\n",
    "data_iterative_tree = imputer_iterative_tree.fit_transform(data)\n",
    "print(f\"\\nIterative Imputation with DecisionTreeRegressor:\")\n",
    "print(pd.DataFrame(data_iterative_tree, columns=['A', 'B', 'C', 'D']))\n",
    "\n",
    "# 3. KNNImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "print(f\"\\n=== KNNImputer Example ===\")\n",
    "imputer_knn = KNNImputer(n_neighbors=2)\n",
    "data_knn = imputer_knn.fit_transform(data)\n",
    "print(f\"KNN Imputation (k=2):\")\n",
    "print(pd.DataFrame(data_knn, columns=['A', 'B', 'C', 'D']))\n",
    "\n",
    "# 4. MissingIndicator\n",
    "from sklearn.impute import MissingIndicator\n",
    "\n",
    "print(f\"\\n=== MissingIndicator Example ===\")\n",
    "indicator = MissingIndicator()\n",
    "missing_mask = indicator.fit_transform(data)\n",
    "print(f\"Missing Value Indicators:\")\n",
    "print(pd.DataFrame(missing_mask, columns=[f'missing_{col}' for col in ['A', 'B', 'C', 'D']]))\n",
    "print(f\"Features with missing values: {indicator.features_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609df532",
   "metadata": {},
   "source": [
    "### Key Considerations for Missing Value Imputation:\n",
    "1. Choose Strategy Based on Data Type:\n",
    "- **Numerical**: Mean (normal distribution), Median (skewed/outliers), KNN (correlated features)\n",
    "- **Categorical**: Most frequent, Constant value\n",
    "2. Consider Missing Data Mechanism:\n",
    "- **MCAR (Missing Completely At Random)**: Simple strategies work well\n",
    "- **MAR (Missing At Random)**: IterativeImputer or KNNImputer better\n",
    "- **MNAR (Missing Not At Random)**: Domain-specific strategies needed\n",
    "3. Evaluation Impact:\n",
    "- Always evaluate model performance with and without imputation\n",
    "- Consider adding missing value indicators as additional features\n",
    "- Be cautious with high missing data percentages (>50%)\n",
    "4. Production Considerations:\n",
    "- Fit imputers only on training data\n",
    "- Save imputation statistics for consistent preprocessing\n",
    "- Handle new missing patterns in production data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b318d5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data with Missing Values (first 10 rows):\n",
      "   Feature_A  Feature_B  Feature_C  Feature_D\n",
      "0   0.496714        NaN        NaN        NaN\n",
      "1  -0.234153        NaN        NaN  -0.564035\n",
      "2        NaN  -0.466853  -1.355897        NaN\n",
      "3   0.241962        NaN   0.676306        NaN\n",
      "4  -1.012831  -1.237864  -2.445888  -2.280514\n",
      "5   1.465649        NaN   3.096585   3.847303\n",
      "6  -0.544383  -1.078193  -1.800294  -2.109810\n",
      "7  -0.600639  -0.671828  -1.531664  -2.029526\n",
      "8  -0.013497   0.046651   0.047710        NaN\n",
      "9   0.208864   0.466083   0.425662   1.024079\n",
      "\n",
      "Missing values per feature:\n",
      "Feature_A    30\n",
      "Feature_B    30\n",
      "Feature_C    30\n",
      "Feature_D    30\n",
      "dtype: int64\n",
      "\n",
      "============================================================\n",
      "ITERATIVE IMPUTATION PROCESS\n",
      "============================================================\n",
      "\n",
      "Step 1: Initial imputation using mean values\n",
      "Initial imputation values: [ 0.09580839 -0.00327686 -0.28212216  0.13757677]\n",
      "\n",
      "--- Iteration 1 ---\n",
      "Feature 0: Predicted 30 missing values\n",
      "  Mean predicted value: -0.070\n",
      "Feature 1: Predicted 30 missing values\n",
      "  Mean predicted value: -0.074\n",
      "Feature 2: Predicted 30 missing values\n",
      "  Mean predicted value: 0.172\n",
      "Feature 3: Predicted 30 missing values\n",
      "  Mean predicted value: -0.428\n",
      "\n",
      "--- Iteration 2 ---\n",
      "Feature 0: Predicted 30 missing values\n",
      "  Mean predicted value: -0.070\n",
      "Feature 1: Predicted 30 missing values\n",
      "  Mean predicted value: -0.074\n",
      "Feature 2: Predicted 30 missing values\n",
      "  Mean predicted value: 0.172\n",
      "Feature 3: Predicted 30 missing values\n",
      "  Mean predicted value: -0.428\n",
      "\n",
      "============================================================\n",
      "USING SKLEARN ITERATIVEIMPUTER\n",
      "============================================================\n",
      "[IterativeImputer] Completing matrix with shape (100, 4)\n",
      "[IterativeImputer] Change: 5.535424313331873, scaled tolerance: 0.004833263628625294 \n",
      "[IterativeImputer] Change: 2.35363643517043, scaled tolerance: 0.004833263628625294 \n",
      "[IterativeImputer] Change: 0.46887517617386754, scaled tolerance: 0.004833263628625294 \n",
      "[IterativeImputer] Change: 0.3815696424629439, scaled tolerance: 0.004833263628625294 \n",
      "[IterativeImputer] Change: 0.2923852719141704, scaled tolerance: 0.004833263628625294 \n",
      "[IterativeImputer] Change: 0.22078098583429606, scaled tolerance: 0.004833263628625294 \n",
      "[IterativeImputer] Change: 0.16344639929440274, scaled tolerance: 0.004833263628625294 \n",
      "[IterativeImputer] Change: 0.11856281035286209, scaled tolerance: 0.004833263628625294 \n",
      "[IterativeImputer] Change: 0.08425056346363924, scaled tolerance: 0.004833263628625294 \n",
      "[IterativeImputer] Change: 0.05860018548775259, scaled tolerance: 0.004833263628625294 \n",
      "\n",
      "Default IterativeImputer completed in 10 iterations\n",
      "\n",
      "BayesianRidge - Iterations: 5\n",
      "\n",
      "LinearRegression - Iterations: 5\n",
      "\n",
      "============================================================\n",
      "COMPARISON OF RESULTS\n",
      "============================================================\n",
      "\n",
      "Mean Squared Error for imputed values:\n",
      "BayesianRidge: 0.1457\n",
      "LinearRegression: 0.1489\n",
      "Simple Mean Imputation: 2.0165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\anaconda3\\envs\\nlp_img\\lib\\site-packages\\sklearn\\impute\\_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "d:\\Develop\\anaconda3\\envs\\nlp_img\\lib\\site-packages\\sklearn\\impute\\_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "d:\\Develop\\anaconda3\\envs\\nlp_img\\lib\\site-packages\\sklearn\\impute\\_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## How IterativeImputer Works\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create example data to demonstrate the iterative process\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "X_complete = np.random.randn(n_samples, 4)\n",
    "# Add some correlation between features\n",
    "X_complete[:, 1] = X_complete[:, 0] + 0.5 * np.random.randn(n_samples)\n",
    "X_complete[:, 2] = X_complete[:, 0] + X_complete[:, 1] + 0.3 * np.random.randn(n_samples)\n",
    "X_complete[:, 3] = X_complete[:, 2] + 0.4 * np.random.randn(n_samples)\n",
    "\n",
    "# Introduce missing values\n",
    "X_missing = X_complete.copy()\n",
    "missing_rate = 0.3\n",
    "for col in range(4):\n",
    "    missing_indices = np.random.choice(n_samples, int(n_samples * missing_rate), replace=False)\n",
    "    X_missing[missing_indices, col] = np.nan\n",
    "\n",
    "df_missing = pd.DataFrame(X_missing, columns=['Feature_A', 'Feature_B', 'Feature_C', 'Feature_D'])\n",
    "print(\"Data with Missing Values (first 10 rows):\")\n",
    "print(df_missing.head(10))\n",
    "print(f\"\\nMissing values per feature:\")\n",
    "print(df_missing.isnull().sum())\n",
    "\n",
    "# Step-by-step demonstration of IterativeImputer\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ITERATIVE IMPUTATION PROCESS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize with simple imputation (mean)\n",
    "from sklearn.impute import SimpleImputer\n",
    "initial_imputer = SimpleImputer(strategy='mean')\n",
    "X_initial = initial_imputer.fit_transform(X_missing)\n",
    "\n",
    "print(\"\\nStep 1: Initial imputation using mean values\")\n",
    "print(\"Initial imputation values:\", initial_imputer.statistics_)\n",
    "\n",
    "# Show the iterative process manually\n",
    "def demonstrate_iteration(X_missing, iteration_num, estimator=LinearRegression()):\n",
    "    print(f\"\\n--- Iteration {iteration_num} ---\")\n",
    "    X_iter = X_initial.copy()\n",
    "    \n",
    "    for feature_idx in range(X_missing.shape[1]):\n",
    "        # Find rows where this feature is missing\n",
    "        missing_mask = np.isnan(X_missing[:, feature_idx])\n",
    "        \n",
    "        if np.any(missing_mask):\n",
    "            # Use other features to predict this feature\n",
    "            other_features = np.delete(np.arange(X_missing.shape[1]), feature_idx)\n",
    "            \n",
    "            # Training data: rows where current feature is NOT missing\n",
    "            train_mask = ~missing_mask\n",
    "            X_train = X_iter[train_mask][:, other_features]\n",
    "            y_train = X_iter[train_mask, feature_idx]\n",
    "            \n",
    "            # Predict missing values\n",
    "            if len(X_train) > 0:\n",
    "                estimator.fit(X_train, y_train)\n",
    "                X_predict = X_iter[missing_mask][:, other_features]\n",
    "                predicted_values = estimator.predict(X_predict)\n",
    "                \n",
    "                print(f\"Feature {feature_idx}: Predicted {len(predicted_values)} missing values\")\n",
    "                print(f\"  Mean predicted value: {predicted_values.mean():.3f}\")\n",
    "                \n",
    "                # Update the missing values\n",
    "                X_iter[missing_mask, feature_idx] = predicted_values\n",
    "    \n",
    "    return X_iter\n",
    "\n",
    "# Demonstrate a few iterations manually\n",
    "X_iter1 = demonstrate_iteration(X_missing, 1)\n",
    "X_iter2 = demonstrate_iteration(X_missing, 2)\n",
    "\n",
    "# Now use the actual IterativeImputer\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"USING SKLEARN ITERATIVEIMPUTER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Default IterativeImputer (uses BayesianRidge)\n",
    "imputer_default = IterativeImputer(random_state=42, max_iter=10, verbose=1)\n",
    "X_imputed_default = imputer_default.fit_transform(X_missing)\n",
    "\n",
    "print(f\"\\nDefault IterativeImputer completed in {imputer_default.n_iter_} iterations\")\n",
    "\n",
    "# IterativeImputer with different estimators\n",
    "estimators = {\n",
    "    'BayesianRidge': BayesianRidge(),\n",
    "    'LinearRegression': LinearRegression(),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, estimator in estimators.items():\n",
    "    imputer = IterativeImputer(estimator=estimator, random_state=42, max_iter=5)\n",
    "    X_imputed = imputer.fit_transform(X_missing)\n",
    "    results[name] = X_imputed\n",
    "    print(f\"\\n{name} - Iterations: {imputer.n_iter_}\")\n",
    "\n",
    "# Compare results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON OF RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate imputation quality (where we know true values)\n",
    "def calculate_imputation_error(X_true, X_missing, X_imputed):\n",
    "    missing_mask = np.isnan(X_missing)\n",
    "    true_values = X_true[missing_mask]\n",
    "    imputed_values = X_imputed[missing_mask]\n",
    "    mse = np.mean((true_values - imputed_values) ** 2)\n",
    "    return mse\n",
    "\n",
    "print(\"\\nMean Squared Error for imputed values:\")\n",
    "for name, X_imputed in results.items():\n",
    "    mse = calculate_imputation_error(X_complete, X_missing, X_imputed)\n",
    "    print(f\"{name}: {mse:.4f}\")\n",
    "\n",
    "# Simple imputation for comparison\n",
    "X_simple = SimpleImputer(strategy='mean').fit_transform(X_missing)\n",
    "mse_simple = calculate_imputation_error(X_complete, X_missing, X_simple)\n",
    "print(f\"Simple Mean Imputation: {mse_simple:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_img",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
